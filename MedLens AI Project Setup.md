# Chat Conversation

Note: _This is purely the output of the chat conversation and does not contain any raw data, codebase snippets, etc. used to generate the output._

### User Input

Vision Possible: Agent Protocol is an online hackathon by WeMakeDevs (part of their Hack All February series), running Feb 23 â€“ Mar 1. The core challenge is to build a multi-modal AI agent that can watch, listen, and understand video in real-time using the Vision Agents SDK by Stream.
Prize pool: $4,000+ plus swag and potential job interviews.

ðŸ› ï¸ What You Need to Build
You must build a real-time Vision AI project â€” something that processes live video intelligently. Some inspiration from the site:

ðŸŒï¸ Golf Coach â€” Real-time pose tracking with YOLO + Gemini for feedback
ðŸ“· Security Camera â€” Face recognition, package detection, theft response
ðŸŒ GeoGuesser â€” OpenAI Realtime identifying real-world locations from video
ðŸ“ž Phone + RAG â€” Twilio integration with retrieval-augmented generation
ðŸŽ¨ Realtime Stable Diffusion â€” Interactive scene generation from live video

You can also come up with anything creative in the real-time Vision AI space â€” security systems, sports coaching, drone detection, interactive gaming, etc.

âš™ï¸ The Required Tech
You MUST use the Vision Agents SDK â€” this is non-negotiable. The more features you use from it, the better your score. Here's what it offers:

Video AI â€” Combine vision models like YOLO, Roboflow, and Moondream with LLMs like Gemini, OpenAI, or Claude
Ultra-low latency â€” Join in 500ms, audio/video latency under 30ms via Stream's edge network
Native LLM APIs â€” Direct access to OpenAI, Gemini, and Claude without wrapper limitations
Multi-platform SDKs â€” Build in React, Android, iOS, Flutter, React Native, or Unity
Processors â€” Manage state and handle audio/video in real-time with pluggable processors
Tool Calling â€” Execute APIs and functions mid-conversation


ðŸ“‹ Rules to Follow

Team size: 1â€“4 people. Teams can vary across hackathons in this series.
You can use starter templates, boilerplates, open-source libraries, and public APIs â€” but you cannot just submit an existing project with minimal changes.
Planning is allowed before the start, but actual coding/design must begin only after the hackathon kicks off. Notes and diagrams beforehand are fine.
Your project must be software-based. Hardware is optional.
All IP created belongs to your team.
Be respectful â€” harassment or discrimination leads to immediate disqualification.


ðŸ† How You'll Be Judged
Your project is evaluated on six criteria:

Potential Impact â€” Does it solve a real, meaningful problem?
Creativity & Innovation â€” Is the idea unique and boundary-pushing?
Technical Excellence â€” Is it well-built with strong use of the SDK?
Real-Time Performance â€” Is it actually low-latency and responsive?
User Experience â€” Is it polished and intuitive?
Best Use of Vision Agents â€” How deeply does it leverage the SDK's capabilities?


ðŸ’° Prizes

ðŸ¥‡ 1st Place (Alpha Protocol): $2,000
ðŸ¥ˆ 2nd Place (Bravo Protocol): $1,500
ðŸ“ Best Blog Post: $500 â€” write about your experience using the Vision Agents SDK
ðŸŽ Top 10 Social Posts: Swag bundles â€” star the Vision Agents GitHub repo and share your progress on social media tagging @VisionAgents
ðŸ’¼ Career Opportunity: Outstanding projects may lead to a role at WeMakeDevs/Stream


ðŸ“š Resources to Get Started

ðŸ“– Vision Agents Docs â€” official getting started guide
ðŸ’» GitHub Repo â€” source code and examples
ðŸ”§ Integrations Guide â€” Gemini, OpenAI, YOLO, Roboflow, Moondream, and 20+ more
ðŸ’¬ Vision Agents Discord â€” for support during the hackathon


âœ… Your Action Checklist

Register via this form
Star the Vision Agents GitHub repo
Read the docs at visionagents.ai and explore the example projects
Plan your idea (notes/diagrams are fine before the start date)
Build your real-time Vision AI agent using the SDK (Feb 23 â€“ Mar 1)
Post on social media about your progress and tag @VisionAgents for swag eligibility
Write a blog post if you want to compete for the $500 blog prize
Submit your project before the deadline


https://www.wemakedevs.org/hackathons/vision
https://www.wemakedevs.org/hackathons/vision/rules
https://www.wemakedevs.org/hackathons/vision/resources

go through these links.. do deep research and give me an idea which will bag the 1st place here..

### User Input

continue

*Read URL content from /hackathons/vision *

*Read URL content from /hackathons/vision/rules *

*Read URL content from /hackathons/vision/resources *

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

### Planner Response

Now let me dive deeper into the Vision Agents SDK itself â€” the GitHub repo and docs â€” to understand what features are available and what existing examples look like.

*Read URL content from /GetStream/Vision-Agents *

*Read URL content from  *

*Read URL content from /integrations *

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

### Planner Response

Now let me look at more SDK capabilities â€” the guides for tool calling, processors, and the video agents quickstart to understand what features can be leveraged deeply.

*Read URL content from /introduction/video-agents *

*Read URL content from /guides/tool-calling *

*Searched web for Vision Agents SDK hackathon winning project ideas real-time video AI 2025 2026*

*Viewed content chunk*

*Viewed content chunk*

*Read URL content from /guides/mcp-tool-calling *

*Read URL content from /guides/video-processors *

*Viewed content chunk*

*Viewed content chunk*

*Viewed content chunk*

### Planner Response

Now I have a comprehensive understanding of the SDK. Let me create a detailed idea document.

*Edited relevant file*

### User Input

ok write down a file where you mention all the resources link if stuck and all the tasks that has to be done in order to complete this project and start to execute and complete the project full which is capable of winning the competition.. when stuck anywhere use the links to navigate and get the answers.. 
connect the repo to a github repo and keep track of the version.. keep pushing after a major feature build soo that it helps in version control


### Planner Response

Let me first check the workspace and then create everything needed.

*Listed directory [new](file:///Users/preet/Developer/new) *

*User accepted the command `git remote -v`*

*User accepted the command `git log --oneline -5 2>/dev/null || echo "No commits yet"`*

*User accepted the command `python3 --version && uv --version 2>/dev/null || echo "uv not found"`*

*User accepted the command `node --version && npm --version 2>/dev/null || echo "Node not found"`*

*Checked command status*

*Viewed [.gitignore](file:///Users/preet/Developer/new/.gitignore) *

*Viewed [.env](file:///Users/preet/Developer/new/.env) *

### Planner Response

I see this repo currently has a different project (PESU slides downloader). I need to create a fresh repo for MedLens AI. Let me first check the remote, then create the comprehensive resources/task doc and implementation plan.

*Checked command status*

*User accepted the command `gh auth status 2>&1 || echo "gh CLI not installed"`*

### Planner Response

Good â€” `gh` CLI is authenticated with the `preeeetham` account. The current repo has an existing project. I need to create a new project directory for MedLens AI. Let me first install `uv` (needed for the Vision Agents SDK), then create the comprehensive task/resources doc and implementation plan.

### Planner Response

Now let me create the comprehensive task.md, resources reference, and implementation plan:

*Edited relevant file*

*Edited relevant file*

### Planner Response

Now let me create the implementation plan and look at the existing examples more carefully to base our code off them:

### Planner Response

Let me now look at a real Vision Agents example to ground the implementation plan in actual working code:

### User Input

use the git account preeeetham

*User accepted the command `curl -LsSf https://astral.sh/uv/install.sh | sh`*

### Planner Response

Good, `uv` is installed. Now let me set up the project directory, initialize with `uv`, and create the GitHub repo:

*User accepted the command `source $HOME/.local/bin/env && mkdir -p /Users/preet/Developer/medlens-ai && cd /Users/preet/Developer/medlens-ai && uv init --python 3.12 .`*

*User accepted the command `source $HOME/.local/bin/env && cd /Users/preet/Developer/medlens-ai && uv add "vision-agents[getstream,gemini,openai,deepgram,elevenlabs,ultralytics]" python-dotenv 2>&1`*

*Checked command status*

### Planner Response

All dependencies installed successfully. Now let me set up git, create the GitHub repo, and write the core project files:

*Edited relevant file*

*Edited relevant file*

### Planner Response

Now let me look at an existing Vision Agents example to ground the code, and create the core agent files:

*Read URL content from /GetStream/Vision-Agents/main/examples/04_security_camera_example/security_camera_example.py *

*Read URL content from /GetStream/Vision-Agents/main/examples/01_simple_agent_example/simple_agent_example.py *

### Planner Response

Got the simple agent example. Now let me also look at the golf coach (uses Realtime + YOLO processor) since it's closest to what we need:

*Read URL content from /GetStream/Vision-Agents/main/examples/02_golf_coach_example/golf_coach_example.py *

*Read URL content from /guides/rag *

*Viewed content chunk*

### Planner Response

Now I have all the reference code I need. Let me use Gemini File Search for RAG (simpler, no extra API key needed) and create the core project files. Let me build everything now:

### Planner Response

Now let me create all the core backend files. I'll create the agent instructions, knowledge base content, custom processor, and main agent entry point:

*Edited relevant file*

*Edited relevant file*

*Edited relevant file*

### Planner Response

Now let me create the main agent file and start building the core backend:

*Edited relevant file*

### Planner Response

Now let me create the README, initialize git, create the GitHub repo, and push: